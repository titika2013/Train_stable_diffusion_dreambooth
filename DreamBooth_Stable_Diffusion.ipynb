{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/titika2013/dreambooth_train/blob/main/DreamBooth_Stable_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU7NuMAA2drw",
        "outputId": "d8c9e7fb-3500-4ce8-8719-ec39342e40b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4, 15109 MiB, 15109 MiB\n"
          ]
        }
      ],
      "source": [
        "#@markdown Check type of GPU and VRAM available.\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzM7j0ZSc_9c"
      },
      "source": [
        "https://github.com/titika2013/dreambooth_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnTMyW41cC1E"
      },
      "source": [
        "## Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "aLWXPZqjsZVV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/titika2013/dreambooth_train\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
        "%pip install -q -U --pre triton\n",
        "%pip install -q accelerate==0.12.0 transformers ftfy bitsandbytes gradio natsort\n",
        "%pip install -q https://github.com/metrolobo/xformers_wheels/releases/download/1d31a3ac_various_6/xformers-0.0.14.dev0-cp37-cp37m-linux_x86_64.whl\n",
        "%pip install -r /content/dreambooth_train/requirements.txt\n",
        "# If precompiled wheels don't work, install it with the following command. It will take around 40 minutes to compile.\n",
        "# %pip install git+https://github.com/facebookresearch/xformers@1d31a3a#egg=xformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "y4lqqWT_uxD2"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.huggingface\n",
        "HUGGINGFACE_TOKEN = \"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\" #@param {type:\"string\"}\n",
        "!echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0NV324ZcL9L"
      },
      "source": [
        "## Settings and run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "Rxg0y5MBudmd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66066a7a-a89a-4381-e6dc-c62d4f855bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Weights will be saved at /content/stable_diffusion_weights/zwx\n"
          ]
        }
      ],
      "source": [
        "#@markdown If model weights should be saved directly in google drive (takes around 4-5 GB).\n",
        "save_to_gdrive = 0\n",
        "#@markdown Name/Path of the initial model.\n",
        "MODELT_NAME = \"runwayml/stable-diffusion-v1-5\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Enter the directory name to save model at.\n",
        "\n",
        "OUTPUT_DIR = \"stable_diffusion_weights/zwx\" #@param {type:\"string\"}\n",
        "if save_to_gdrive:\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image prepare"
      ],
      "metadata": {
        "id": "KGkQzZ7LQS3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeyXTCkOQeLo",
        "outputId": "6b6bf881-5f9c-4b6a-90ea-e12c6d0d1a47"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('/content/dreambooth_train')"
      ],
      "metadata": {
        "id": "FiaPs2J4V4Zv"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGES_FOLDER = \"/content/dreambooth_train/image_crop.py\" #@param{type: 'string'}\n"
      ],
      "metadata": {
        "id": "6JXXmNwqkSZw"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python \"$IMAGES_FOLDER\" --image_path \"/content/drive/MyDrive/fresh_photos\" --save_image_path \"my_photos\" --crop_size 512 --face_finder True"
      ],
      "metadata": {
        "id": "u7-YGtMESyA6"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn5ILIyDJIcX"
      },
      "source": [
        "# Start Training\n",
        "\n",
        "Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n",
        "\n",
        "\n",
        "| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n",
        "| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n",
        "| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n",
        "| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n",
        "| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n",
        "| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n",
        "| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n",
        "| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n",
        "| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ioxxvHoicPs"
      },
      "source": [
        "Add `--gradient_checkpointing` flag for around 9.92 GB VRAM usage.\n",
        "\n",
        "remove `--use_8bit_adam` flag for full precision. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB.\n",
        "\n",
        "remove `--train_text_encoder` flag to reduce memory usage further, degrades output quality."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "#@markdown - If you accidentally run the old method cell below, you need to run this cell again but no need to reupload images if they are already uploaded.\n",
        "Session_Name = \"new_session\" #@param{type: 'string'}\n",
        "while Session_Name==\"\":\n",
        "  print('\u001b[1;31mInput the Session Name:') \n",
        "  Session_Name=input('')\n",
        "INSTANCE_NAME=Session_Name\n",
        "#@markdown - Create or Load a session, just enter its name, it if it exists, it will load it, otherwise it'll create an new session.\n",
        "WORKSPACE='/content/'\n",
        "OUTPUT_DIR=\"/content/models/\"+Session_Name\n",
        "SESSION_DIR=WORKSPACE+'/Sessions/'+Session_Name\n",
        "INSTANCE_DIR=WORKSPACE+\"/Sessions/\"+Session_Name+\"/instance_images\"\n",
        "MDLPTH=str(SESSION_DIR+\"/\"+Session_Name+'.ckpt')\n"
      ],
      "metadata": {
        "id": "W5VIOA7nenGh"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "5vDpCxId1aCm"
      },
      "outputs": [],
      "source": [
        "# You can also add multiple concepts here. Try tweaking `--max_train_steps` accordingly.\n",
        "inst_prompt = INSTANCE_NAME\n",
        "class_prompt = \"inst_prompt\"\n",
        "instanse_path = \"/content/my_photos\"\n",
        "class_path_dir = INSTANCE_DIR\n",
        "concepts_list = [\n",
        "    {\n",
        "        \"instance_prompt\":      f\"{inst_prompt}\",\n",
        "        \"class_prompt\":         f\"{class_prompt}\",\n",
        "        \"instance_data_dir\":    f\"{instanse_path}\",\n",
        "        \"class_data_dir\":       f\"{instanse_path}\",\n",
        "    },\n",
        "]\n",
        "\n",
        "# `class_data_dir` contains regularization images\n",
        "import json\n",
        "import os\n",
        "for c in concepts_list:\n",
        "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
        "\n",
        "with open(\"concepts_list.json\", \"w\") as f:\n",
        "    json.dump(concepts_list, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Update_repo = False #@param {type:\"boolean\"}\n",
        "\n",
        "INSTANCE__NAME=\"new_session\" #@param{type: 'string'}\n",
        "\n",
        "#@markdown - Or Session_Name, Leave empty if you want to use the current trained model.\n",
        "\n",
        "if INSTANCE__NAME!=\"\":\n",
        "  INSTANCET=INSTANCE__NAME\n",
        "try:\n",
        "  INSTANCE_NAME\n",
        "  INSTANCET=INSTANCE_NAME  \n",
        "except:\n",
        "  pass\n",
        "#@markdown - if checked, an input box will ask the full path to a desired model\n",
        "\n"
      ],
      "metadata": {
        "id": "jVOHRZ2rSjEz"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.utils import capture\n",
        "#@markdown - If you prefer to specify directly the folder of the pictures instead of uploading, this will add the pictures to the existing (if any) instance images. Leave EMPTY to upload.\n",
        "while IMAGES_FOLDER !=\"\" and not os.path.exists(str(IMAGES_FOLDER)):\n",
        "  print('\u001b[1;31mThe image folder specified does not exist, use the colab file explorer to copy the path :')\n",
        "  IMAGES_FOLDER=input('')\n",
        "if IMAGES_FOLDER!=\"\":\n",
        "  with capture.capture_output() as cap:\n",
        "    %cp -r \"$IMAGES_FOLDER/.\" \"$INSTANCE_DIR\"\n",
        "    %cd \"$INSTANCE_DIR\"\n",
        "    !find . -name \"* *\" -type f | rename 's/ /_/g'\n",
        "    %cd /content\n",
        "    if os.path.exists(INSTANCE_DIR+\"/.ipynb_checkpoints\"):\n",
        "      %rm -r INSTANCE_DIR+\"/.ipynb_checkpoints\"      \n",
        "  print('\u001b[1;32mDone, proceed to the training cell')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqVfBal4kJiG",
        "outputId": "a153436c-9c62-4fc6-edba-6ad3b83b0413"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;32mDone, proceed to the training cell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Training_Steps=3000 #@param{type: 'number'}\n",
        "Save_Checkpoint_Every=500 #@param{type: 'number'}\n",
        "Start_saving_from_the_step=500 #@param{type: 'number'}\n",
        "\n",
        "SEED = 1313\n",
        "if Start_saving_from_the_step==None:\n",
        "  Start_saving_from_the_step=0\n",
        "if (Start_saving_from_the_step < 200):\n",
        "  Start_saving_from_the_step=Save_Checkpoint_Every\n",
        "stpsv=Start_saving_from_the_step\n",
        "\n",
        "Train_text_encoder_for=40 #@param{type: 'number'}\n",
        "\n",
        "#@markdown - Keep the % low for better style transfer, more training steps will be necessary for good results.\n",
        "#@markdown - Higher % will give more weight to the instance, it gives stronger results at lower steps count, but harder to stylize, \n",
        "\n",
        "if Train_text_encoder_for==100:\n",
        "  stptxt=Training_Steps+10\n",
        "elif Train_text_encoder_for==0:\n",
        "  Enable_text_encoder_training= False\n",
        "  stptxt=10\n",
        "else:\n",
        "  stptxt=int((Training_Steps*Train_text_encoder_for)/100)"
      ],
      "metadata": {
        "id": "ZvT0Ia_lnQYo"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjcSXTp-u-Eg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74a745b8-f962-4fcf-a4b3-ad1231a9d2b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--num_cpu_threads_per_process` was set to `1` to improve out-of-box performance\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Caching latents: 100% 50/50 [00:13<00:00,  3.70it/s]\n",
            "Steps:   6% 186/3000 [03:10<45:14,  1.04it/s, loss=0.271, lr=1.89e-6]"
          ]
        }
      ],
      "source": [
        "!accelerate launch /content/dreambooth_train/train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=\"$MODEL_NAME\" \\\n",
        "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
        "  --revision=\"fp16\" \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --seed=1337 \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --train_text_encoder \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=2e-6 \\\n",
        "  --lr_scheduler=\"polynomial\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --num_class_images=50 \\\n",
        "  --sample_batch_size=4 \\\n",
        "  --max_train_steps=\"$Training_Steps\" \\\n",
        "  --save_interval=10000 \\\n",
        "  --save_sample_prompt=\"%INSTANCE__NAME\" \\\n",
        "  --concepts_list=\"concepts_list.json\"\n",
        "\n",
        "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
        "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89Az5NUxOWdy"
      },
      "outputs": [],
      "source": [
        "#@markdown Specify the weights directory to use (leave blank for latest)\n",
        "WEIGHTS_DIR = \"my_wieght\" #@param {type:\"string\"}\n",
        "if WEIGHTS_DIR == \"\":\n",
        "    from natsort import natsorted\n",
        "    from glob import glob\n",
        "    import os\n",
        "    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
        "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UDHPfUmMBxN"
      },
      "outputs": [],
      "source": [
        "#@markdown Run to generate a grid of preview images from the last saved weights.\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "weights_folder = OUTPUT_DIR\n",
        "folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n",
        "\n",
        "row = len(folders)\n",
        "col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n",
        "scale = 4\n",
        "fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n",
        "\n",
        "for i, folder in enumerate(folders):\n",
        "    folder_path = os.path.join(weights_folder, folder)\n",
        "    image_folder = os.path.join(folder_path, \"samples\")\n",
        "    images = [f for f in os.listdir(image_folder)]\n",
        "    for j, image in enumerate(images):\n",
        "        if row == 1:\n",
        "            currAxes = axes[j]\n",
        "        else:\n",
        "            currAxes = axes[i, j]\n",
        "        if i == 0:\n",
        "            currAxes.set_title(f\"Image {j}\")\n",
        "        if j == 0:\n",
        "            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n",
        "        image_path = os.path.join(image_folder, image)\n",
        "        img = mpimg.imread(image_path)\n",
        "        currAxes.imshow(img, cmap='gray')\n",
        "        currAxes.axis('off')\n",
        "        \n",
        "plt.tight_layout()\n",
        "plt.savefig('grid.png', dpi=72)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}